{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fall/anaconda3/envs/ldms_311/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/ipykernel_1230904/2058275332.py:32: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  diffusion_model.load_state_dict(torch.load(checkpoint_path, map_location=device), strict=False)\n",
      "/tmp/ipykernel_1230904/2058275332.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  clear_encoder_state_dict = torch.load(clear_encoder_weight_path, map_location=device)\n",
      "/tmp/ipykernel_1230904/2058275332.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  blur_encoder_state_dict = torch.load(blur_encoder_weight_path, map_location=device)\n",
      "/tmp/ipykernel_1230904/2058275332.py:48: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  decoder_state_dict = torch.load(decoder_weight_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned models successfully loaded.\n"
     ]
    }
   ],
   "source": [
    "import model_loader\n",
    "import pipeline_copy\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from transformers import CLIPTokenizer\n",
    "from encoder import VAE_Encoder\n",
    "from decoder import VAE_Decoder\n",
    "from diffusion_copy import Diffusion\n",
    "from torchvision import transforms\n",
    "\n",
    "import torch\n",
    "import os\n",
    "\n",
    "DEVICE = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Tokenizer 설정\n",
    "tokenizer = CLIPTokenizer(\"../data/vocab.json\", merges_file=\"../data/merges.txt\")\n",
    "model_file = \"/home/NAS_mount/seunghan/v1-5-pruned-emaonly.ckpt\"\n",
    "\n",
    "# 저장된 모델 파일 경로\n",
    "checkpoint_path = \"/home/fall/latent-diffusion-homemade/ldms/checkpoints/model_epoch_33.pth\"\n",
    "models = model_loader.preload_models_from_standard_weights(model_file, DEVICE)\n",
    "\n",
    "# Fine-tuned Encoder 및 Decoder 가중치 경로\n",
    "clear_encoder_weight_path = \"/home/fall/latent-diffusion-homemade/ldms/checkpoints/sharp_encoder_bilinear_epoch_100.pth\"\n",
    "blur_encoder_weight_path =\"/home/fall/latent-diffusion-homemade/ldms/checkpoints/blur_encoder_conv_epoch_220.pth\"\n",
    "decoder_weight_path = \"/home/fall/latent-diffusion-homemade/ldms/checkpoints/sharp_decoder_bilinear_epoch_100.pth\"\n",
    "# 모델 로드\n",
    "def load_finetuned_model(models, checkpoint_path, clear_encoder_weight_path, blur_encoder_weight_path, decoder_weight_path, device):    \n",
    "    # Diffusion 모델에 저장된 가중치 적용\n",
    "    diffusion_model = Diffusion().to(device)\n",
    "    diffusion_model.load_state_dict(torch.load(checkpoint_path, map_location=device), strict=False)\n",
    "    models['diffusion'] = diffusion_model.to(device)\n",
    "    \n",
    "    # Encoder와 Decoder 로드 및 추가\n",
    "    clear_encoder = VAE_Encoder().to(device)\n",
    "    blur_encoder = VAE_Encoder().to(device)\n",
    "    decoder = VAE_Decoder().to(device)\n",
    "    \n",
    "    # 가중치 로드 함수\n",
    "    def load_state_dict_without_module(model, state_dict):\n",
    "        new_state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "        model.load_state_dict(new_state_dict)\n",
    "    \n",
    "    # Encoder와 Decoder 가중치 로드\n",
    "    clear_encoder_state_dict = torch.load(clear_encoder_weight_path, map_location=device)\n",
    "    blur_encoder_state_dict = torch.load(blur_encoder_weight_path, map_location=device)\n",
    "    decoder_state_dict = torch.load(decoder_weight_path, map_location=device)\n",
    "    load_state_dict_without_module(clear_encoder, clear_encoder_state_dict)\n",
    "    load_state_dict_without_module(blur_encoder, blur_encoder_state_dict)\n",
    "    load_state_dict_without_module(decoder, decoder_state_dict)\n",
    "    \n",
    "    # 모델 딕셔너리에 Encoder와 Decoder 추가\n",
    "    models['clear encoder'] = clear_encoder\n",
    "    models['blur encoder'] = blur_encoder\n",
    "    models['decoder'] = decoder\n",
    "        \n",
    "    return models\n",
    "\n",
    "# 저장된 모델 로드\n",
    "models = load_finetuned_model(models, checkpoint_path, clear_encoder_weight_path, blur_encoder_weight_path, decoder_weight_path, DEVICE)\n",
    "\n",
    "print(\"Fine-tuned models successfully loaded.\")\n",
    "\n",
    "\n",
    "def center_crop(image, crop_size):\n",
    "    width, height = image.size\n",
    "    new_width, new_height = crop_size\n",
    "    left = (width - new_width) // 2\n",
    "    top = (height - new_height) // 2\n",
    "    right = (width + new_width) // 2\n",
    "    bottom = (height + new_height) // 2\n",
    "    return image.crop((left, top, right, bottom))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latent shape: torch.Size([2, 4, 32, 32]), blur_latent shape: torch.Size([2, 4, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [00:00<00:11,  4.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latent shape: torch.Size([2, 4, 32, 32]), blur_latent shape: torch.Size([4, 4, 32, 32])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 2 but got size 4 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 28\u001b[0m\n\u001b[1;32m     24\u001b[0m output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../images/output_deblurred_image.png\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# 이미지 생성\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m output_image \u001b[38;5;241m=\u001b[39m pipeline_copy\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m     29\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclean image\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     30\u001b[0m     uncond_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# condition_image=input_image,\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     strength\u001b[38;5;241m=\u001b[39mstrength,\n\u001b[1;32m     33\u001b[0m     do_cfg\u001b[38;5;241m=\u001b[39mdo_cfg,\n\u001b[1;32m     34\u001b[0m     cfg_scale\u001b[38;5;241m=\u001b[39mcfg_scale,\n\u001b[1;32m     35\u001b[0m     sampler_name\u001b[38;5;241m=\u001b[39msampler,\n\u001b[1;32m     36\u001b[0m     n_inference_steps\u001b[38;5;241m=\u001b[39mnum_inference_steps,\n\u001b[1;32m     37\u001b[0m     seed\u001b[38;5;241m=\u001b[39mseed,\n\u001b[1;32m     38\u001b[0m     models\u001b[38;5;241m=\u001b[39mmodels\u001b[38;5;241m.\u001b[39mmodule \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(models, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mDataParallel) \u001b[38;5;28;01melse\u001b[39;00m models,\n\u001b[1;32m     39\u001b[0m     device\u001b[38;5;241m=\u001b[39mDEVICE,\n\u001b[1;32m     40\u001b[0m     idle_device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     41\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[1;32m     42\u001b[0m )\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# 결과 이미지 저장\u001b[39;00m\n\u001b[1;32m     45\u001b[0m output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../images/output_deblurred_image.png\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/latent-diffusion-homemade/ldms/pipeline_copy.py:166\u001b[0m, in \u001b[0;36mgenerate\u001b[0;34m(prompt, uncond_prompt, input_image, condition_image, strength, do_cfg, cfg_scale, sampler_name, n_inference_steps, models, seed, device, idle_device, tokenizer)\u001b[0m\n\u001b[1;32m    162\u001b[0m     blur_latents \u001b[38;5;241m=\u001b[39m blur_latents\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# model_output is the predicted noise\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# (Batch_Size, 4, Latents_Height, Latents_Width) -> (Batch_Size, 4, Latents_Height, Latents_Width)\u001b[39;00m\n\u001b[0;32m--> 166\u001b[0m model_output \u001b[38;5;241m=\u001b[39m diffusion(model_input, blur_latents, context, time_embedding)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_cfg:\n\u001b[1;32m    169\u001b[0m     output_cond, output_uncond \u001b[38;5;241m=\u001b[39m model_output\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/ldms_311/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/ldms_311/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/latent-diffusion-homemade/ldms/diffusion_copy.py:342\u001b[0m, in \u001b[0;36mDiffusion.forward\u001b[0;34m(self, latent, blur_latent, context, time)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, latent, blur_latent, context, time):\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;66;03m# latent: (Batch_Size, 4, Height / 8, Width / 8)\u001b[39;00m\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;66;03m# context: (Batch_Size, Seq_Len, Dim)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;66;03m# Combine latent and blur_image along channel dimension\u001b[39;00m\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;66;03m# (Batch_Size, 4, Height / 8, Width / 8) + (Batch_Size, 4, Height / 8, Width / 8) -> (Batch_Size, 8, Height / 8, Width / 8)\u001b[39;00m\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatent shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlatent\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, blur_latent shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mblur_latent\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 342\u001b[0m     concated_latent \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((latent, blur_latent), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;66;03m# (1, 320) -> (1, 1280)\u001b[39;00m\n\u001b[1;32m    346\u001b[0m     time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_embedding(time)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 2 but got size 4 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "\n",
    "# 이미지 디블러링을 위한 샘플링 파라미터\n",
    "do_cfg = True\n",
    "cfg_scale = 8  # Prompt에 집중하는 정도\n",
    "image_path = \"../images/blur_image.png\"  # 블러 이미지 경로\n",
    "\n",
    "# 이미지 로드\n",
    "input_image = Image.open(image_path)\n",
    "input_image = center_crop(input_image, (256, 256))\n",
    "# 이미지 중앙 부분 crop\n",
    "transform = transforms.Compose([\n",
    "    transforms.CenterCrop((256, 256)), \n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "input_tensor = transform(input_image).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "strength = 0.8 # Higher: More noise, Lower: Less noise\n",
    "\n",
    "# 샘플러 설정\n",
    "sampler = \"ddpm\"\n",
    "num_inference_steps = 50\n",
    "seed = 42\n",
    "\n",
    "output_path = \"../images/output_deblurred_image.png\"\n",
    "\n",
    "\n",
    "# 이미지 생성\n",
    "output_image = pipeline_copy.generate(\n",
    "    prompt = \"clean image\",\n",
    "    uncond_prompt = \"\",\n",
    "    # condition_image=input_image,\n",
    "    strength=strength,\n",
    "    do_cfg=do_cfg,\n",
    "    cfg_scale=cfg_scale,\n",
    "    sampler_name=sampler,\n",
    "    n_inference_steps=num_inference_steps,\n",
    "    seed=seed,\n",
    "    models=models.module if isinstance(models, torch.nn.DataParallel) else models,\n",
    "    device=DEVICE,\n",
    "    idle_device=\"cpu\",\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# 결과 이미지 저장\n",
    "output_path = \"../images/output_deblurred_image.png\"\n",
    "Image.fromarray(output_image).save(output_path)\n",
    "print(f\"Deblurred image saved at {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(output_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ldms_311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
