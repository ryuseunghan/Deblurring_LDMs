{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from encoder import VAE_Encoder\n",
    "from decoder import VAE_Decoder\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import model_loader\n",
    "\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import lpips\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch import nn, optim\n",
    "\n",
    "from torchvision.utils import save_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1205042/3036001571.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  encoder_state_dict = torch.load(encoder_weight_path, map_location=DEVICE)\n",
      "/tmp/ipykernel_1205042/3036001571.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  decoder_state_dict = torch.load(decoder_weight_path, map_location=DEVICE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned encoder and decoder successfully loaded.\n"
     ]
    }
   ],
   "source": [
    "# 디바이스 설정\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 모델 정의\n",
    "encoder = VAE_Encoder().to(DEVICE)\n",
    "decoder = VAE_Decoder().to(DEVICE)\n",
    "\n",
    "# Fine-tuned 모델 가중치 파일 경로\n",
    "encoder_weight_path = \"/home/fall/latent-diffusion-homemade/ldms/checkpoints/blur_encoder_conv_epoch_220.pth\"\n",
    "decoder_weight_path = \"/home/fall/latent-diffusion-homemade/ldms/checkpoints/blur_decoder_conv_epoch_220.pth\"\n",
    "\n",
    "def load_state_dict_without_module(model, state_dict):\n",
    "    new_state_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        name = k.replace(\"module.\", \"\")  # Remove the 'module.' prefix\n",
    "        new_state_dict[name] = v\n",
    "    model.load_state_dict(new_state_dict)\n",
    "\n",
    "# Load encoder weights\n",
    "encoder_state_dict = torch.load(encoder_weight_path, map_location=DEVICE)\n",
    "load_state_dict_without_module(encoder, encoder_state_dict)\n",
    "\n",
    "# Load decoder weights\n",
    "decoder_state_dict = torch.load(decoder_weight_path, map_location=DEVICE)\n",
    "load_state_dict_without_module(decoder, decoder_state_dict)\n",
    "\n",
    "print(\"Fine-tuned encoder and decoder successfully loaded.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fall/anaconda3/envs/ldms_311/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/fall/anaconda3/envs/ldms_311/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /home/fall/anaconda3/envs/ldms_311/lib/python3.11/site-packages/lpips/weights/v0.1/alex.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fall/anaconda3/envs/ldms_311/lib/python3.11/site-packages/lpips/lpips.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(model_path, map_location='cpu'), strict=False)\n"
     ]
    }
   ],
   "source": [
    "# def preprocess_image(image_path, target_size=(256, 256)):\n",
    "#     image = Image.open(image_path).convert(\"RGB\")\n",
    "#     image = image.crop(target_size)  # Autoencoder 입력 크기로 조정\n",
    "#     image = np.array(image).astype(np.float32) / 255.0  # Normalize to [0, 1]\n",
    "#     image = torch.tensor(image).permute(2, 0, 1).unsqueeze(0).to(DEVICE)  # (H, W, C) -> (1, C, H, W)\n",
    "    \n",
    "#     return image\n",
    "\n",
    "def reconstruct_image(encoder, decoder, image):\n",
    "    with torch.no_grad():\n",
    "        # Encoder로 Latent 공간으로 변환\n",
    "        noise = torch.randn(image.shape[0], 4, image.shape[2] // 8, image.shape[3] // 8).to(DEVICE)\n",
    "        latents = encoder(image, noise)\n",
    "        \n",
    "        # Decoder로 Latent를 복원\n",
    "        reconstructed_image = decoder(latents)\n",
    "        reconstructed_image = reconstructed_image.clamp(0, 1)  # 값 제한 [0, 1]\n",
    "        \n",
    "        return reconstructed_image\n",
    "    \n",
    "def compute_mse(original, reconstructed):\n",
    "    return ((original - reconstructed) ** 2).mean().item()\n",
    "\n",
    "\n",
    "def compute_ssim(input_image, reconstructed_image):\n",
    "    # 디버깅용 출력\n",
    "    print(\"Input image shape:\", input_image.shape)\n",
    "    print(\"Reconstructed image shape:\", reconstructed_image.shape)\n",
    "\n",
    "    # 텐서를 numpy 배열로 변환 및 차원 변환 (C, H, W -> H, W, C)\n",
    "    input_image_np = input_image.squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "    reconstructed_image_np = reconstructed_image.squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "    # SSIM 계산\n",
    "    ssim_value = ssim(\n",
    "        input_image_np,\n",
    "        reconstructed_image_np,\n",
    "        data_range=1.0,        # 정규화된 값 [0, 1] 범위로 설정\n",
    "        multichannel=True,     # 다중 채널 이미지 처리\n",
    "        channel_axis=-1        # 채널 축 위치 (-1은 마지막 축)\n",
    "    )\n",
    "    return ssim_value\n",
    "\n",
    "lpips_loss = lpips.LPIPS(net=\"alex\").to(DEVICE)\n",
    "\n",
    "def compute_lpips(original, reconstructed):\n",
    "    return lpips_loss(original, reconstructed).item()\n",
    "\n",
    "def reconstruction_loss(original, reconstructed):\n",
    "    return F.mse_loss(reconstructed, original)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 이미지 경로\n",
    "image_path = \"/home/fall/latent-diffusion-homemade/images/blur_image.png\"\n",
    "\n",
    "# 이미지 전처리\n",
    "input_image = preprocess_image(image_path)\n",
    "\n",
    "# Reconstruction\n",
    "reconstructed_image = reconstruct_image(encoder, decoder, input_image)\n",
    "\n",
    "# 성능 평가\n",
    "mse = compute_mse(input_image, reconstructed_image)\n",
    "ssim_value = compute_ssim(input_image, reconstructed_image)\n",
    "lpips_value = compute_lpips(input_image, reconstructed_image)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"MSE: {mse:.4f}, SSIM: {ssim_value:.4f}, LPIPS: {lpips_value:.4f}\")\n",
    "\n",
    "# 복원된 이미지 저장\n",
    "reconstructed_image_np = reconstructed_image.squeeze().cpu().numpy().transpose(1, 2, 0) * 255.0\n",
    "reconstructed_image_np = reconstructed_image_np.astype(np.uint8)\n",
    "Image.fromarray(reconstructed_image_np).save(\"reconstructed_image.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GOPRODataset(Dataset):\n",
    "    def __init__(self, root_dir, mode='train', transform=None):\n",
    "        \"\"\"\n",
    "        GOPRODataset 생성자\n",
    "        :param root_dir: 데이터셋의 최상위 디렉토리 경로\n",
    "        :param mode: 'train' 또는 'test'\n",
    "        :param transform: 이미지 전처리 변환\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "\n",
    "        # 허용된 이미지 확장자\n",
    "        valid_extensions = {\".jpg\", \".jpeg\", \".png\"}\n",
    "\n",
    "        # 폴더 탐색 및 blur, sharp 이미지 경로 저장\n",
    "        base_dir = os.path.join(root_dir, mode)\n",
    "        for subdir in os.listdir(base_dir):\n",
    "            blur_dir = os.path.join(base_dir, subdir, 'blur')\n",
    "            sharp_dir = os.path.join(base_dir, subdir, 'sharp')\n",
    "            if os.path.exists(blur_dir) and os.path.exists(sharp_dir):\n",
    "                blur_images = sorted(os.listdir(blur_dir))\n",
    "                sharp_images = sorted(os.listdir(sharp_dir))\n",
    "                for blur_img, sharp_img in zip(blur_images, sharp_images):\n",
    "                    if os.path.splitext(blur_img)[1].lower() in valid_extensions and \\\n",
    "                       os.path.splitext(sharp_img)[1].lower() in valid_extensions:\n",
    "                        self.data.append({\n",
    "                            'blur': os.path.join(blur_dir, blur_img),\n",
    "                            'sharp': os.path.join(sharp_dir, sharp_img)\n",
    "                        })\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        blur_image = Image.open(item['blur']).convert('RGB')\n",
    "        sharp_image = Image.open(item['sharp']).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            blur_image = self.transform(blur_image)\n",
    "            sharp_image = self.transform(sharp_image)\n",
    "\n",
    "        return blur_image, sharp_image\n",
    "\n",
    "# 이미지 전처리\n",
    "transform = transforms.Compose([\n",
    "    transforms.CenterCrop((512, 512)),  # AE 입력 크기로 조정\n",
    "    transforms.ToTensor(),         # [0, 1] 범위로 정규화\n",
    "])\n",
    "\n",
    "# 데이터셋 경로\n",
    "root_dir = \"/home/NAS_mount/seunghan/GOPRO/\"\n",
    "\n",
    "# 데이터셋 생성\n",
    "train_dataset = GOPRODataset(root_dir=root_dir, mode='train', transform=transform)\n",
    "test_dataset = GOPRODataset(root_dir=root_dir, mode='test', transform=transform)\n",
    "\n",
    "# DataLoader 생성\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'tuple' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m latents \u001b[38;5;241m=\u001b[39m encoder(sharp_images, noise)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# 복원 이미지 생성 (Decoder)\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m reconstructed \u001b[38;5;241m=\u001b[39m decoder(latents)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# 손실 계산 (Reconstruction Loss)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss(reconstructed, sharp_images)\n",
      "File \u001b[0;32m~/anaconda3/envs/ldms_311/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/ldms_311/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/latent-diffusion-homemade/ldms/decoder.py:171\u001b[0m, in \u001b[0;36mVAE_Decoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# x: (Batch_Size, 4, Height / 8, Width / 8)\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \n\u001b[1;32m    170\u001b[0m     \u001b[38;5;66;03m# Remove the scaling added by the Encoder.\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m0.18215\u001b[39m\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m    174\u001b[0m         x \u001b[38;5;241m=\u001b[39m module(x)\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'tuple' and 'float'"
     ]
    }
   ],
   "source": [
    "# 모델 로드 (encoder, decoder는 미리 로드되어 있다고 가정)\n",
    "# 모델을 평가 모드로 설정\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# 테스트 평가 루프\n",
    "total_loss = 0\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for blur_images, sharp_images in test_loader:\n",
    "        # DataLoader에서 반환된 데이터를 GPU로 전송\n",
    "        sharp_images = sharp_images.to(DEVICE)\n",
    "        \n",
    "        # Latent 공간으로 변환 (Encoder)\n",
    "        noise = torch.randn(sharp_images.size(0), 4, sharp_images.size(2) // 8, sharp_images.size(3) // 8).to(DEVICE)\n",
    "        latents = encoder(sharp_images, noise)\n",
    "        \n",
    "        # 복원 이미지 생성 (Decoder)\n",
    "        reconstructed = decoder(latents)\n",
    "        \n",
    "        # 손실 계산 (Reconstruction Loss)\n",
    "        loss = nn.MSELoss(reconstructed, sharp_images)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Test Loss: {total_loss / len(test_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 복원된 이미지 저장\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m blur_images, _ \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[1;32m      4\u001b[0m         blur_images \u001b[38;5;241m=\u001b[39m blur_images\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m      6\u001b[0m         noise \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(blur_images\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m4\u001b[39m, blur_images\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m8\u001b[39m, blur_images\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m3\u001b[39m) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m8\u001b[39m)\u001b[38;5;241m.\u001b[39mto(DEVICE)\n",
      "File \u001b[0;32m~/anaconda3/envs/ldms_311/lib/python3.11/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/anaconda3/envs/ldms_311/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1448\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1448\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_data()\n\u001b[1;32m   1449\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1451\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ldms_311/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1412\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1408\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1409\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1410\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1411\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1412\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_get_data()\n\u001b[1;32m   1413\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1414\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/ldms_311/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1243\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1230\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1231\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1232\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1241\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1242\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1243\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_queue\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m   1244\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1245\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1246\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1247\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1248\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ldms_311/lib/python3.11/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[1;32m    112\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout):\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[0;32m~/anaconda3/envs/ldms_311/lib/python3.11/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout)\n",
      "File \u001b[0;32m~/anaconda3/envs/ldms_311/lib/python3.11/multiprocessing/connection.py:440\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 440\u001b[0m     r \u001b[38;5;241m=\u001b[39m wait([\u001b[38;5;28mself\u001b[39m], timeout)\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m~/anaconda3/envs/ldms_311/lib/python3.11/multiprocessing/connection.py:948\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    945\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    947\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 948\u001b[0m     ready \u001b[38;5;241m=\u001b[39m selector\u001b[38;5;241m.\u001b[39mselect(timeout)\n\u001b[1;32m    949\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    950\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m~/anaconda3/envs/ldms_311/lib/python3.11/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selector\u001b[38;5;241m.\u001b[39mpoll(timeout)\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 복원된 이미지 저장\n",
    "with torch.no_grad():\n",
    "    for blur_images, _ in test_loader:\n",
    "        blur_images = blur_images.to(DEVICE)\n",
    "        \n",
    "        noise = torch.randn(blur_images.size(0), 4, blur_images.size(2) // 8, blur_images.size(3) // 8).to(DEVICE)\n",
    "        latents, _, _ = encoder(blur_images, noise)\n",
    "        reconstructed = decoder(latents).clamp(0, 1)\n",
    "        \n",
    "        save_image(reconstructed, \"blur_reconstructed_images.png\")\n",
    "        break  # 한 배치만 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 전처리 및 모델 설정\n",
    "with torch.no_grad():\n",
    "    for blur_images, sharp_images in test_loader:\n",
    "        sharp_images = sharp_images.to(DEVICE)\n",
    "        \n",
    "        # Encoder와 Decoder를 통해 복원된 이미지 생성\n",
    "        noise = torch.randn(sharp_images.size(0), 4, sharp_images.size(2) // 8, sharp_images.size(3) // 8).to(DEVICE)\n",
    "        latents,_,_ = encoder(sharp_images, noise)\n",
    "        reconstructed = decoder(latents).clamp(0, 1)\n",
    "\n",
    "        # 세로 방향으로 이미지를 나란히 저장하기 위해 두 이미지를 합침\n",
    "        # dim=2는 세로 방향으로 이미지를 합침\n",
    "        comparison_image = torch.cat((sharp_images, reconstructed), dim=2)  # 세로로 이어붙이기\n",
    "        \n",
    "        # 이미지를 하나의 파일로 저장\n",
    "        save_image(comparison_image, \"100_bilinear_sharp_vs_reconstructed_vertical.png\")\n",
    "        break  # 한 배치만 저장\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 전처리 및 모델 설정\n",
    "with torch.no_grad():\n",
    "    for blur_images, sharp_images in test_loader:\n",
    "        blur_images = blur_images.to(DEVICE)\n",
    "        \n",
    "        # Encoder와 Decoder를 통해 복원된 이미지 생성\n",
    "        noise = torch.randn(blur_images.size(0), 4, blur_images.size(2) // 8, blur_images.size(3) // 8).to(DEVICE)\n",
    "        latents,_,_ = encoder(blur_images, noise)\n",
    "        reconstructed = decoder(latents).clamp(0, 1)\n",
    "\n",
    "        # 세로 방향으로 이미지를 나란히 저장하기 위해 두 이미지를 합침\n",
    "        # dim=2는 세로 방향으로 이미지를 합침\n",
    "        comparison_image = torch.cat((blur_images, reconstructed), dim=2)  # 세로로 이어붙이기\n",
    "        \n",
    "        # 이미지를 하나의 파일로 저장\n",
    "        save_image(comparison_image, \"220_bilinear_blur_vs_reconstructed_vertical.png\")\n",
    "        break  # 한 배치만 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GOPRODataset(Dataset):\n",
    "    def __init__(self, root_dir, mode='train', transform=None):\n",
    "        \"\"\"\n",
    "        GOPRODataset 생성자\n",
    "        :param root_dir: 데이터셋의 최상위 디렉토리 경로\n",
    "        :param mode: 'train' 또는 'test'\n",
    "        :param transform: 이미지 전처리 변환\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "\n",
    "        # 폴더 탐색 및 blur, sharp 이미지 경로 저장\n",
    "        base_dir = os.path.join(root_dir, mode)\n",
    "        for subdir in os.listdir(base_dir):\n",
    "            blur_dir = os.path.join(base_dir, subdir, 'blur')\n",
    "            sharp_dir = os.path.join(base_dir, subdir, 'sharp')\n",
    "            if os.path.exists(blur_dir) and os.path.exists(sharp_dir):\n",
    "                blur_images = sorted(os.listdir(blur_dir))\n",
    "                sharp_images = sorted(os.listdir(sharp_dir))\n",
    "                for blur_img, sharp_img in zip(blur_images, sharp_images):\n",
    "                    self.data.append({\n",
    "                        'blur': os.path.join(blur_dir, blur_img),\n",
    "                        'sharp': os.path.join(sharp_dir, sharp_img)\n",
    "                    })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        blur_image = Image.open(item['blur']).convert('RGB')\n",
    "        sharp_image = Image.open(item['sharp']).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            blur_image = self.transform(blur_image)\n",
    "            sharp_image = self.transform(sharp_image)\n",
    "\n",
    "        return blur_image, sharp_image\n",
    "\n",
    "# 이미지 전처리\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),  # AE 입력 크기로 조정\n",
    "    transforms.ToTensor(),         # [0, 1] 범위로 정규화\n",
    "])\n",
    "\n",
    "# 데이터셋 경로\n",
    "root_dir = \"/home/NAS_mount/seunghan/GOPRO/\"\n",
    "\n",
    "# 데이터셋 생성\n",
    "train_dataset = GOPRODataset(root_dir=root_dir, mode='train', transform=transform)\n",
    "test_dataset = GOPRODataset(root_dir=root_dir, mode='test', transform=transform)\n",
    "\n",
    "# DataLoader 생성\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# 입력 이미지\n",
    "image_path = \"/home/fall/latent-diffusion-homemade/images/blur_image.png\"\n",
    "image = transform(Image.open(image_path).convert(\"RGB\")).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "# Reconstruction\n",
    "with torch.no_grad():\n",
    "    noise = torch.randn(1, 4, 512 // 8, 512 // 8).to(DEVICE)\n",
    "    latent = encoder(image, noise)\n",
    "    reconstructed_image = decoder(latent).clamp(0, 1)\n",
    "\n",
    "# 이미지 저장\n",
    "from torchvision.utils import save_image\n",
    "save_image(reconstructed_image, \"reconstructed_image.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from encoder import VAE_Encoder\n",
    "from decoder import VAE_Decoder\n",
    "\n",
    "# 모델 초기화\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder = VAE_Encoder().to(device)\n",
    "decoder = VAE_Decoder().to(device)\n",
    "\n",
    "# 손실 함수 및 옵티마이저\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 학습 루프\n",
    "# num_epochs = 100\n",
    "# for epoch in range(num_epochs):\n",
    "#     encoder.train()\n",
    "#     decoder.train()\n",
    "#     total_loss = 0\n",
    "\n",
    "#     for blur_images, sharp_images in train_loader:\n",
    "#         blur_images, sharp_images = blur_images.to(device), sharp_images.to(device)\n",
    "        \n",
    "#         # Forward pass\n",
    "#         noise = torch.randn(blur_images.size(0), 4, blur_images.size(2) // 8, blur_images.size(3) // 8).to(device)\n",
    "#         latents = encoder(blur_images, noise)\n",
    "#         reconstructed = decoder(latents)\n",
    "        \n",
    "#         # Reconstruction Loss\n",
    "#         loss = criterion(reconstructed, sharp_images)\n",
    "\n",
    "#         # Backward pass\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#     # Epoch 결과 출력\n",
    "#     print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 테스트 루프\n",
    "# encoder.eval()\n",
    "# decoder.eval()\n",
    "# total_loss = 0\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for blur_images, sharp_images in test_loader:\n",
    "#         blur_images, sharp_images = blur_images.to(device), sharp_images.to(device)\n",
    "        \n",
    "#         # Forward pass\n",
    "#         noise = torch.randn(blur_images.size(0), 4, blur_images.size(2) // 8, blur_images.size(3) // 8).to(device)\n",
    "#         latents = encoder(blur_images, noise)\n",
    "#         reconstructed = decoder(latents)\n",
    "\n",
    "#         # Reconstruction Loss\n",
    "#         loss = criterion(reconstructed, sharp_images)\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#     print(f\"Test Loss: {total_loss / len(test_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import save_image\n",
    "\n",
    "# 복원된 이미지 저장\n",
    "with torch.no_grad():\n",
    "    for blur_images, sharp_images in test_loader:\n",
    "        blur_images = blur_images.to(device)\n",
    "        \n",
    "        noise = torch.randn(blur_images.size(0), 4, blur_images.size(2) // 8, blur_images.size(3) // 8).to(device)\n",
    "        latents = encoder(blur_images, noise)\n",
    "        reconstructed = decoder(latents).clamp(0, 1)\n",
    "        \n",
    "        save_image(reconstructed, \"reconstructed_images.png\")\n",
    "        break  # 한 배치만 저장\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fall/anaconda3/envs/ldms_311/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'encoder'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m decoder\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# 가중치 로드\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m encoder\u001b[38;5;241m.\u001b[39mload_state_dict(models[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoder\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstate_dict(), strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     40\u001b[0m decoder\u001b[38;5;241m.\u001b[39mload_state_dict(models[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecoder\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstate_dict(), strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'encoder'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn  \n",
    "from encoder import VAE_Encoder\n",
    "from decoder import VAE_Decoder\n",
    "import model_loader\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch import nn, optim\n",
    "from contperceptual import LPIPSWithDiscriminator\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "DEVICE = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 미리 학습된 모델 파일 경로\n",
    "model_file = \"/home/NAS_mount/seunghan/v1-5-pruned-emaonly.ckpt\"\n",
    "\n",
    "# 모델 로드\n",
    "models = model_loader.preload_models_from_standard_weights(model_file, DEVICE)\n",
    "\n",
    "# Encoder와 Decoder 초기화\n",
    "encoder = VAE_Encoder()\n",
    "decoder = VAE_Decoder()\n",
    "\n",
    "# DataParallel로 Multi-GPU 활용\n",
    "if torch.cuda.device_count() > 1:\n",
    "    encoder = nn.DataParallel(encoder)\n",
    "    decoder = nn.DataParallel(decoder)\n",
    "\n",
    "# 모델을 GPU로 이동\n",
    "encoder.to(DEVICE)\n",
    "decoder.to(DEVICE)\n",
    "\n",
    "# 가중치 로드\n",
    "encoder.load_state_dict(models['encoder'].state_dict(), strict=False)\n",
    "decoder.load_state_dict(models['decoder'].state_dict(), strict=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/300:   0%|          | 0/132 [22:35<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300], Loss: 0.0268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/300:   0%|          | 0/132 [24:17<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/300], Loss: 0.0108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/300:   0%|          | 0/132 [33:46<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/300], Loss: 0.0079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/300:   0%|          | 0/132 [23:28<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 106\u001b[0m\n\u001b[1;32m    104\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(train_loader, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m tepoch:\n\u001b[0;32m--> 106\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m blur_images, sharp_images \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m    107\u001b[0m         sharp_images \u001b[38;5;241m=\u001b[39m sharp_images\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m    109\u001b[0m         \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ldms_311/lib/python3.11/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/anaconda3/envs/ldms_311/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1448\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1448\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_data()\n\u001b[1;32m   1449\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1451\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ldms_311/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1412\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1408\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1409\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1410\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1411\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1412\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_get_data()\n\u001b[1;32m   1413\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1414\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/ldms_311/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1243\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1230\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1231\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1232\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1241\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1242\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1243\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_queue\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m   1244\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1245\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1246\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1247\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1248\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ldms_311/lib/python3.11/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[1;32m    112\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout):\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[0;32m~/anaconda3/envs/ldms_311/lib/python3.11/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout)\n",
      "File \u001b[0;32m~/anaconda3/envs/ldms_311/lib/python3.11/multiprocessing/connection.py:440\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 440\u001b[0m     r \u001b[38;5;241m=\u001b[39m wait([\u001b[38;5;28mself\u001b[39m], timeout)\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m~/anaconda3/envs/ldms_311/lib/python3.11/multiprocessing/connection.py:948\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    945\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    947\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 948\u001b[0m     ready \u001b[38;5;241m=\u001b[39m selector\u001b[38;5;241m.\u001b[39mselect(timeout)\n\u001b[1;32m    949\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    950\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m~/anaconda3/envs/ldms_311/lib/python3.11/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selector\u001b[38;5;241m.\u001b[39mpoll(timeout)\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%env PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n",
    "# from contperceptual import LPIPSWithDiscriminator\n",
    "from tqdm import tqdm\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# KL-divergence와 MSE 혼합 손실 함수\n",
    "kl_weight = 1e-6 # KL-divergence의 가중치\n",
    "\n",
    "# 옵티마이저\n",
    "optimizer = optim.Adam(\n",
    "    list(encoder.parameters()) + list(decoder.parameters()), \n",
    "    lr=1e-4,         # 학습률\n",
    "    weight_decay=1e-4  # 가중치 감쇠 (옵션, 필요 시 제거 가능)\n",
    ")\n",
    "\n",
    "\n",
    "# optimizer = optim.SGD(\n",
    "#     list(encoder.parameters()) + list(decoder.parameters()), \n",
    "#     lr=1e-3,         # 학습률 (SGD는 일반적으로 Adam보다 더 높은 학습률 사용)\n",
    "#     momentum=0.9,    # 모멘텀 (옵션)\n",
    "#     weight_decay=1e-4  # 가중치 감쇠 (L2 정규화, 옵션)\n",
    "# )\n",
    "\n",
    "# optimizer_weight_path = \"/home/fall/latent-diffusion-homemade/ldms/checkpoints/sharp_optimizer_encoder_decoder_bilinear_epoch_100.pth\"\n",
    "# optimizer_state_dict = torch.load(optimizer_weight_path, map_location=DEVICE)\n",
    "# optimizer.load_state_dict(optimizer_state_dict)\n",
    "\n",
    "\n",
    "class GOPRODataset(Dataset):\n",
    "    def __init__(self, root_dir, mode='train', transform=None):\n",
    "        \"\"\"\n",
    "        GOPRODataset 생성자\n",
    "        :param root_dir: 데이터셋의 최상위 디렉토리 경로\n",
    "        :param mode: 'train' 또는 'test'\n",
    "        :param transform: 이미지 전처리 변환\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "\n",
    "        # 허용된 이미지 확장자\n",
    "        valid_extensions = {\".jpg\", \".jpeg\", \".png\"}\n",
    "\n",
    "        # 폴더 탐색 및 blur, sharp 이미지 경로 저장\n",
    "        base_dir = os.path.join(root_dir, mode)\n",
    "        for subdir in os.listdir(base_dir):\n",
    "            blur_dir = os.path.join(base_dir, subdir, 'blur')\n",
    "            sharp_dir = os.path.join(base_dir, subdir, 'sharp')\n",
    "            if os.path.exists(blur_dir) and os.path.exists(sharp_dir):\n",
    "                blur_images = sorted(os.listdir(blur_dir))\n",
    "                sharp_images = sorted(os.listdir(sharp_dir))\n",
    "                for blur_img, sharp_img in zip(blur_images, sharp_images):\n",
    "                    if os.path.splitext(blur_img)[1].lower() in valid_extensions and \\\n",
    "                       os.path.splitext(sharp_img)[1].lower() in valid_extensions:\n",
    "                        self.data.append({\n",
    "                            'blur': os.path.join(blur_dir, blur_img),\n",
    "                            'sharp': os.path.join(sharp_dir, sharp_img)\n",
    "                        })\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        blur_image = Image.open(item['blur']).convert('RGB')\n",
    "        sharp_image = Image.open(item['sharp']).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            blur_image = self.transform(blur_image)\n",
    "            sharp_image = self.transform(sharp_image)\n",
    "\n",
    "        return blur_image, sharp_image\n",
    "\n",
    "# 이미지 전처리\n",
    "transform = transforms.Compose([\n",
    "    transforms.CenterCrop((256, 256)),  # AE 입력 크기로 조정\n",
    "    transforms.ToTensor(),         # [0, 1] 범위로 정규화\n",
    "])\n",
    "\n",
    "# 데이터셋 경로\n",
    "root_dir = \"/home/NAS_mount/seunghan/GOPRO/\"\n",
    "\n",
    "# 데이터셋 생성\n",
    "train_dataset = GOPRODataset(root_dir=root_dir, mode='train', transform=transform)\n",
    "# test_dataset = GOPRODataset(root_dir=root_dir, mode='test', transform=transform)\n",
    "\n",
    "# DataLoader 생성\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=1)\n",
    "\n",
    "# 학습 루프\n",
    "num_epochs = 300\n",
    "# 학습 루프 수정\n",
    "for epoch in range(num_epochs):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    total_loss = 0\n",
    "    with tqdm(train_loader, unit=\"batch\", desc=f\"Epoch {epoch+1}/{num_epochs}\") as tepoch:\n",
    "        for blur_images, sharp_images in train_loader:\n",
    "            sharp_images = sharp_images.to(DEVICE)\n",
    "            \n",
    "            # Forward pass\n",
    "            noise = torch.randn(sharp_images.size(0), 4, sharp_images.size(2) // 8, sharp_images.size(3) // 8).to(DEVICE)\n",
    "            latents, mean, log_variance = encoder(sharp_images, noise)\n",
    "            reconstructed = decoder(latents)\n",
    "\n",
    "            \n",
    "            # Reconstruction Loss\n",
    "            mse_loss  = criterion(reconstructed, sharp_images)\n",
    "            # KL-divergence Loss\n",
    "            kl_loss = torch.mean(-0.5 * torch.sum(1 + log_variance - mean.pow(2) - log_variance.exp(), dim=1))\n",
    "\n",
    "            # Combined Loss\n",
    "            loss = mse_loss + kl_weight * kl_loss\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # 모델 저장 (50 에포크마다)\n",
    "    if (epoch + 1) % 25 == 0:\n",
    "        torch.save(encoder.state_dict(), f\"/home/fall/latent-diffusion-homemade/ldms/checkpoints/sharpencoder_bilinear_epoch_{epoch+1}.pth\")\n",
    "        torch.save(decoder.state_dict(), f\"/home/fall/latent-diffusion-homemade/ldms/checkpoints/sharpdecoder_bilinear_epoch_{epoch+1}.pth\")\n",
    "        torch.save(optimizer.state_dict(), f\"/home/fall/latent-diffusion-homemade/ldms/checkpoints/sharpoptimizer_encoder_decoder_bilinear_epoch_{epoch+1}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터셋 로드\n",
    "test_dataset = GOPRODataset(root_dir=root_dir, mode='test', transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4)\n",
    "\n",
    "# 평가\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "total_loss = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for blur_images, sharp_images in test_loader:\n",
    "        blur_images, sharp_images = blur_images.to(DEVICE), sharp_images.to(DEVICE)\n",
    "        \n",
    "        # Forward pass\n",
    "        noise = torch.randn(blur_images.size(0), 4, blur_images.size(2) // 8, blur_images.size(3) // 8).to(DEVICE)\n",
    "        latents = encoder(blur_images, noise)\n",
    "        reconstructed = decoder(latents)\n",
    "\n",
    "        # Reconstruction Loss\n",
    "        loss = criterion(reconstructed, blur_images)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Test Loss: {total_loss / len(test_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import save_image\n",
    "\n",
    "# 복원된 이미지 저장\n",
    "with torch.no_grad():\n",
    "    for blur_images in test_loader:\n",
    "        blur_images = blur_images.to(DEVICE)\n",
    "        \n",
    "        noise = torch.randn(blur_images.size(0), 4, blur_images.size(2) // 8, blur_images.size(3) // 8).to(DEVICE)\n",
    "        latents = encoder(blur_images, noise)\n",
    "        reconstructed = decoder(latents).clamp(0, 1)\n",
    "        \n",
    "        save_image(reconstructed, \"blur_reconstructed_images.png\")\n",
    "        break  # 한 배치만 저장\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clear encoder decoder 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn  \n",
    "from encoder import VAE_Encoder\n",
    "from decoder import VAE_Decoder\n",
    "import model_loader\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 미리 학습된 모델 파일 경로\n",
    "model_file = \"/home/NAS_mount/seunghan/v1-5-pruned-emaonly.ckpt\"\n",
    "\n",
    "# 모델 로드\n",
    "models = model_loader.preload_models_from_standard_weights(model_file, DEVICE)\n",
    "\n",
    "# Encoder와 Decoder 초기화\n",
    "encoder = VAE_Encoder()\n",
    "decoder = VAE_Decoder()\n",
    "\n",
    "# DataParallel로 Multi-GPU 활용\n",
    "if torch.cuda.device_count() > 1:\n",
    "    encoder = nn.DataParallel(encoder)\n",
    "    decoder = nn.DataParallel(decoder)\n",
    "\n",
    "# 모델을 GPU로 이동\n",
    "encoder.to(DEVICE)\n",
    "decoder.to(DEVICE)\n",
    "\n",
    "# 가중치 로드\n",
    "encoder.load_state_dict(models['encoder'].state_dict(), strict=False)\n",
    "decoder.load_state_dict(models['decoder'].state_dict(), strict=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n",
    "\n",
    "# 손실 함수 및 옵티마이저 정의\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(\n",
    "    list(encoder.parameters()) + list(decoder.parameters()), \n",
    "    lr=1e-3,         # 학습률 (SGD는 일반적으로 Adam보다 더 높은 학습률 사용)\n",
    "    momentum=0.9,    # 모멘텀 (옵션)\n",
    "    weight_decay=1e-4  # 가중치 감쇠 (L2 정규화, 옵션)\n",
    ")\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "class GOPRODataset(Dataset):\n",
    "    def __init__(self, root_dir, mode='train', transform=None):\n",
    "        \"\"\"\n",
    "        GOPRODataset 생성자\n",
    "        :param root_dir: 데이터셋의 최상위 디렉토리 경로\n",
    "        :param mode: 'train' 또는 'test'\n",
    "        :param transform: 이미지 전처리 변환\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "\n",
    "        # 허용된 이미지 확장자\n",
    "        valid_extensions = {\".jpg\", \".jpeg\", \".png\"}\n",
    "\n",
    "        # 폴더 탐색 및 blur, sharp 이미지 경로 저장\n",
    "        base_dir = os.path.join(root_dir, mode)\n",
    "        for subdir in os.listdir(base_dir):\n",
    "            blur_dir = os.path.join(base_dir, subdir, 'blur')\n",
    "            sharp_dir = os.path.join(base_dir, subdir, 'sharp')\n",
    "            if os.path.exists(blur_dir) and os.path.exists(sharp_dir):\n",
    "                blur_images = sorted(os.listdir(blur_dir))\n",
    "                sharp_images = sorted(os.listdir(sharp_dir))\n",
    "                for blur_img, sharp_img in zip(blur_images, sharp_images):\n",
    "                    if os.path.splitext(blur_img)[1].lower() in valid_extensions and \\\n",
    "                       os.path.splitext(sharp_img)[1].lower() in valid_extensions:\n",
    "                        self.data.append({\n",
    "                            'blur': os.path.join(blur_dir, blur_img),\n",
    "                            'sharp': os.path.join(sharp_dir, sharp_img)\n",
    "                        })\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        blur_image = Image.open(item['blur']).convert('RGB')\n",
    "        sharp_image = Image.open(item['sharp']).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            blur_image = self.transform(blur_image)\n",
    "            sharp_image = self.transform(sharp_image)\n",
    "\n",
    "        return blur_image, sharp_image\n",
    "\n",
    "# 이미지 전처리\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),  # AE 입력 크기로 조정\n",
    "    transforms.ToTensor(),         # [0, 1] 범위로 정규화\n",
    "])\n",
    "\n",
    "# 데이터셋 경로\n",
    "root_dir = \"/home/NAS_mount/seunghan/GOPRO/\"\n",
    "\n",
    "# 데이터셋 생성\n",
    "train_dataset = GOPRODataset(root_dir=root_dir, mode='train', transform=transform)\n",
    "# test_dataset = GOPRODataset(root_dir=root_dir, mode='test', transform=transform)\n",
    "\n",
    "# DataLoader 생성\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=1)\n",
    "\n",
    "# 학습 루프\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for _, sharp_images in train_loader:\n",
    "        sharp_images = sharp_images.to(DEVICE)\n",
    "        \n",
    "        # Forward pass\n",
    "        noise = torch.randn(sharp_images.size(0), 4, sharp_images.size(2) // 8, sharp_images.size(3) // 8).to(DEVICE)\n",
    "        latents = encoder(sharp_images, noise)\n",
    "        reconstructed = decoder(latents)\n",
    "        \n",
    "        # Reconstruction Loss\n",
    "        loss = criterion(reconstructed, sharp_images)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # 모델 저장 (10 에포크마다)\n",
    "    if (epoch + 1) % 25 == 0:\n",
    "        torch.save(encoder.state_dict(), f\"fine_tuned_CLEAR_encoder_epoch_{epoch+101}.pth\")\n",
    "        torch.save(decoder.state_dict(), f\"fine_tuned_CLEAR_decoder_epoch_{epoch+101}.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "total_loss = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for blur_images, sharp_images in test_loader:\n",
    "        sharp_images = sharp_images.to(DEVICE)\n",
    "        \n",
    "        # Forward pass\n",
    "        noise = torch.randn(sharp_images.size(0), 4, sharp_images.size(2) // 8, sharp_images.size(3) // 8).to(DEVICE)\n",
    "        latents = encoder(sharp_images, noise)\n",
    "        reconstructed = decoder(latents)\n",
    "\n",
    "        # Reconstruction Loss\n",
    "        loss = criterion(reconstructed, sharp_images)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Test Loss: {total_loss / len(test_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 전처리 및 모델 설정\n",
    "with torch.no_grad():\n",
    "    for blur_images, sharp_images in test_loader:\n",
    "        sharp_images = sharp_images.to(DEVICE)\n",
    "        \n",
    "        # Encoder와 Decoder를 통해 복원된 이미지 생성\n",
    "        noise = torch.randn(sharp_images.size(0), 4, sharp_images.size(2) // 8, sharp_images.size(3) // 8).to(DEVICE)\n",
    "        latents = encoder(sharp_images, noise)\n",
    "        reconstructed = decoder(latents).clamp(0, 1)\n",
    "\n",
    "        # 세로 방향으로 이미지를 나란히 저장하기 위해 두 이미지를 합침\n",
    "        # dim=2는 세로 방향으로 이미지를 합침\n",
    "        comparison_image = torch.cat((sharp_images, reconstructed), dim=2)  # 세로로 이어붙이기\n",
    "        \n",
    "        # 이미지를 하나의 파일로 저장\n",
    "        save_image(comparison_image, \"sharp_vs_reconstructed_vertical.png\")\n",
    "        break  # 한 배치만 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ldms_311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
