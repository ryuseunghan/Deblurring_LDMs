{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fall/anaconda3/envs/ldms_311/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import model_loader\n",
    "import pipeline\n",
    "from PIL import Image\n",
    "from transformers import CLIPTokenizer\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "import torch\n",
    "from torch.optim import  SGD\n",
    "from tqdm import tqdm\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.transforms import functional as TF\n",
    "\n",
    "\n",
    "from clip import CLIP\n",
    "from ddpm import DDPMSampler\n",
    "from pipeline import generate, get_time_embedding\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from encoder import VAE_Encoder\n",
    "from decoder import VAE_Decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0 on GPUs [0, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "# GPU 설정\n",
    "device_ids = [0, 1, 2, 3]  \n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# print(f\"Using device: {device} on GPUs {device_ids}\")\n",
    "\n",
    "device = f\"cuda:{device_ids[0]}\"  # 첫 번째 GPU를 기본 device로 설정\n",
    "print(f\"Using device: {device} on GPUs {device_ids}\")\n",
    "\n",
    "# 토크나이저 및 모델 로드\n",
    "tokenizer = CLIPTokenizer(\"../data/vocab.json\", merges_file=\"../data/merges.txt\")\n",
    "model_file = \"/home/NAS_mount/seunghan/v1-5-pruned-emaonly.ckpt\"\n",
    "models = model_loader.preload_models_from_standard_weights(model_file, device)\n",
    "\n",
    "# Diffusion 모델 병렬화 설정\n",
    "model = models[\"diffusion\"]\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = torch.nn.DataParallel(model, device_ids=device_ids)  # 모델 병렬화\n",
    "# model = model.to(device)\n",
    "model = model  # .to(device) 필요 없음, DataParallel이 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from encoder import VAE_Encoder\n",
    "from decoder import VAE_Decoder\n",
    "import model_loader\n",
    "from transformers import CLIPTokenizer\n",
    "\n",
    "# GPU 설정\n",
    "device_ids = [0, 1, 2, 3]\n",
    "device = f\"cuda:{device_ids[0]}\"  # 첫 번째 GPU를 기본 device로 설정\n",
    "print(f\"Using device: {device} on GPUs {device_ids}\")\n",
    "\n",
    "# 토크나이저 및 Pre-trained 모델 로드\n",
    "tokenizer = CLIPTokenizer(\"../data/vocab.json\", merges_file=\"../data/merges.txt\")\n",
    "model_file = \"/home/NAS_mount/seunghan/v1-5-pruned-emaonly.ckpt\"\n",
    "\n",
    "# Fine-tuned Encoder 및 Decoder 가중치 경로\n",
    "encoder_weight_path = \"/home/fall/latent-diffusion-homemade/ldms/checkpoints/fine_tuned_encoder_epoch_100.pth\"\n",
    "decoder_weight_path = \"/home/fall/latent-diffusion-homemade/ldms/checkpoints/fine_tuned_decoder_epoch_100.pth\"\n",
    "\n",
    "# Fine-tuned 모델 로드 함수\n",
    "def load_finetuned_model(model_file, encoder_weight_path, decoder_weight_path, device, device_ids):\n",
    "    # Pretrained 모델 로드\n",
    "    models = model_loader.preload_models_from_standard_weights(model_file, device)\n",
    "\n",
    "    # Encoder와 Decoder 초기화\n",
    "    encoder = VAE_Encoder().to(device)\n",
    "    decoder = VAE_Decoder().to(device)\n",
    "\n",
    "    # 가중치 로드 함수\n",
    "    def load_state_dict_without_module(model, state_dict):\n",
    "        new_state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "        model.load_state_dict(new_state_dict, strict=False)\n",
    "\n",
    "    # Fine-tuned 가중치 로드\n",
    "    encoder_state_dict = torch.load(encoder_weight_path, map_location=device)\n",
    "    decoder_state_dict = torch.load(decoder_weight_path, map_location=device)\n",
    "    load_state_dict_without_module(encoder, encoder_state_dict)\n",
    "    load_state_dict_without_module(decoder, decoder_state_dict)\n",
    "\n",
    "    # Encoder와 Decoder를 모델 딕셔너리에 추가\n",
    "    models['encoder'] = encoder\n",
    "    models['decoder'] = decoder\n",
    "\n",
    "    # Multi-GPU 처리\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"Using {torch.cuda.device_count()} GPUs for parallel processing\")\n",
    "        for key in models:\n",
    "            if isinstance(models[key], torch.nn.Module):\n",
    "                models[key] = torch.nn.DataParallel(models[key], device_ids=device_ids)\n",
    "\n",
    "    return models\n",
    "\n",
    "# Fine-tuned 모델 로드\n",
    "models = load_finetuned_model(\n",
    "    model_file=model_file,\n",
    "    encoder_weight_path=encoder_weight_path,\n",
    "    decoder_weight_path=decoder_weight_path,\n",
    "    device=device,\n",
    "    device_ids=device_ids\n",
    ")\n",
    "\n",
    "# Diffusion 모델 Multi-GPU 설정\n",
    "diffusion_model = models[\"diffusion\"]\n",
    "if torch.cuda.device_count() > 1:\n",
    "    diffusion_model = torch.nn.DataParallel(diffusion_model, device_ids=device_ids)\n",
    "\n",
    "print(\"Models successfully loaded and configured.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class PairedImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.pairs = self._make_pairs()\n",
    "\n",
    "    def _make_pairs(self):\n",
    "        pairs = []\n",
    "        valid_extensions = {\".jpg\", \".jpeg\", \".png\"}  # 유효한 이미지 확장자\n",
    "\n",
    "        # 각 폴더를 순회하면서 blur와 sharp 이미지 쌍을 생성합니다.\n",
    "        for folder_name in os.listdir(self.root_dir):\n",
    "            folder_path = os.path.join(self.root_dir, folder_name)\n",
    "            blur_dir = os.path.join(folder_path, \"blur\")\n",
    "            sharp_dir = os.path.join(folder_path, \"sharp\")\n",
    "            \n",
    "            if not (os.path.isdir(blur_dir) and os.path.isdir(sharp_dir)):\n",
    "                continue\n",
    "            \n",
    "            # blur와 sharp 디렉토리에서 동일한 파일 이름을 가진 이미지 쌍을 찾습니다.\n",
    "            for image_name in os.listdir(blur_dir):\n",
    "                # 유효한 이미지 파일만 선택\n",
    "                if not any(image_name.lower().endswith(ext) for ext in valid_extensions):\n",
    "                    continue\n",
    "                \n",
    "                blur_image_path = os.path.join(blur_dir, image_name)\n",
    "                sharp_image_path = os.path.join(sharp_dir, image_name)\n",
    "                \n",
    "                # 두 파일 모두 존재할 때만 추가\n",
    "                if os.path.isfile(blur_image_path) and os.path.isfile(sharp_image_path):\n",
    "                    pairs.append((blur_image_path, sharp_image_path))\n",
    "        \n",
    "        return pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        blur_image_path, sharp_image_path = self.pairs[idx]\n",
    "        blur_image = Image.open(blur_image_path).convert(\"RGB\")\n",
    "        sharp_image = Image.open(sharp_image_path).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            blur_image = self.transform(blur_image)\n",
    "            sharp_image = self.transform(sharp_image)\n",
    "        \n",
    "        return blur_image, sharp_image  # (blurred input, sharp target)\n",
    "\n",
    "# 이미지 중앙 부분 crop\n",
    "transform = transforms.Compose([\n",
    "    transforms.CenterCrop((512, 512)), \n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# 데이터셋 및 데이터 로더\n",
    "train_data = PairedImageDataset(\"/home/NAS_mount/seunghan/GOPRO/train/\", transform=transform)\n",
    "train_loader = DataLoader(train_data, batch_size=1, shuffle=True)\n",
    "\n",
    "# Test 데이터셋과 DataLoader 생성\n",
    "# test_dataset = PairedImageDataset(\"../images/GOPRO/test\", transform=transform)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vgg16\n",
    "from torchvision.transforms import Normalize\n",
    "\n",
    "class PerceptualLoss(torch.nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super().__init__()\n",
    "        # VGG 모델의 feature extractor를 사용\n",
    "        self.vgg = vgg16(pretrained=True).features[:16].to(device).eval()\n",
    "        for param in self.vgg.parameters():\n",
    "            param.requires_grad = False  # VGG 파라미터를 고정\n",
    "        \n",
    "        # VGG 입력 정규화를 위한 Normalize\n",
    "        self.normalize = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        # VGG 입력 정규화 및 특징 추출\n",
    "        pred = self.normalize(pred)\n",
    "        target = self.normalize(target)\n",
    "        pred_features = self.vgg(pred)\n",
    "        target_features = self.vgg(target)\n",
    "        \n",
    "        # Perceptual Loss (특징 차이의 L2 Norm)\n",
    "        loss = torch.nn.functional.mse_loss(pred_features, target_features)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIDTH = 256\n",
    "HEIGHT = 256\n",
    "LATENTS_WIDTH = WIDTH // 8\n",
    "LATENTS_HEIGHT = HEIGHT // 8\n",
    "\n",
    "class LDMFineTuner:\n",
    "    def __init__(self, models, tokenizer, device, log_dir=\"runs/ldm_finetune\", save_path=\"./checkpoints\"):\n",
    "        self.models = models\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.device = device\n",
    "        self.clip = models['clip'].to(device)\n",
    "        self.diffusion = models['diffusion'].to(device)\n",
    "        self.encoder = models['encoder'].to(device)\n",
    "        self.decoder = models['decoder'].to(device)\n",
    "        \n",
    "        # 옵티마이저 설정 (Diffusion 모델만 학습)\n",
    "        self.optimizer = torch.optim.SGD(self.diffusion.parameters(), lr=1e-4)\n",
    "        \n",
    "        self.sampler = DDPMSampler(generator=torch.Generator(device=device))\n",
    "        self.sampler.set_inference_timesteps(num_inference_steps=1000) \n",
    "\n",
    "        # TensorBoard writer 설정\n",
    "        self.writer = SummaryWriter(log_dir=log_dir)\n",
    "        \n",
    "        # 모델과 옵티마이저 저장 경로 설정\n",
    "        self.save_path = save_path\n",
    "        os.makedirs(self.save_path, exist_ok=True)  # 디렉토리 생성\n",
    "\n",
    "        # Initialize PerceptualLoss\n",
    "        self.perceptual_loss_fn = PerceptualLoss(device).to(device)        \n",
    "\n",
    "\n",
    "    def prepare_batch(self, batch):\n",
    "        blur_images, sharp_images = batch\n",
    "        blur_images = blur_images.to(self.device)\n",
    "        sharp_images = sharp_images.to(self.device)\n",
    "\n",
    "        # 블러 이미지와 선명한 이미지에 대한 다운샘플링된 노이즈 생성\n",
    "        batch_size, _, height, width = blur_images.size()\n",
    "        noise_height, noise_width = height // 8, width // 8  # 다운샘플링 크기에 맞게 설정\n",
    "        noise_for_blur = torch.randn(batch_size, 4, noise_height, noise_width, device=self.device)\n",
    "        noise_for_sharp = torch.zeros(batch_size, 4, noise_height, noise_width, device=self.device)\n",
    "\n",
    "        # 블러 이미지를 latent space로 인코딩\n",
    "        with torch.no_grad():\n",
    "            blur_latents = self.encoder(blur_images, noise_for_blur)\n",
    "            sharp_latents = self.encoder(sharp_images, noise_for_sharp)  # Target latents for deblurring\n",
    "        \n",
    "        # CLIP을 사용하여 텍스트 임베딩 생성 (기본 프롬프트 사용)\n",
    "        # 여기서는 임의의 \"Deblur image\" 프롬프트를 사용하여 context 생성\n",
    "        tokens = self.tokenizer([\"Deblur image\"] * blur_images.size(0), \n",
    "                                padding=\"max_length\", max_length=77, \n",
    "                                return_tensors=\"pt\").input_ids.to(self.device)\n",
    "        context = self.clip(tokens)\n",
    "        \n",
    "        return blur_latents, sharp_latents, context\n",
    "    \n",
    "    def train_step(self, blur_latents, sharp_latents, context):\n",
    "        batch_size = blur_latents.shape[0]\n",
    "        \n",
    "        # 랜덤 타임스텝 선택\n",
    "        t = torch.randint(0, self.sampler.num_train_timesteps, (batch_size,), device=self.device).long()\n",
    "        \n",
    "        # 타깃 latents에 노이즈 추가\n",
    "        noise = torch.randn_like(sharp_latents)\n",
    "        noisy_sharp_latents = self.sampler.add_noise(sharp_latents, t)\n",
    "\n",
    "        # 블러 latent에 노이즈 추가\n",
    "        # noisy_blur_latents = self.sampler.add_noise(blur_latents, t)  # sharp image만을 사용하기 위해 주석처리 \n",
    "        \n",
    "        # 시간 임베딩\n",
    "        time_embedding = get_time_embedding(t).to(self.device)\n",
    "        \n",
    "        # Diffusion 모델로 노이즈 예측\n",
    "        # predicted_noise = self.diffusion(noisy_blur_latents, context, time_embedding) # sharp image만을 사용하기 위해 주석처리 및 아래로 수정\n",
    "        predicted_noise = self.diffusion(noisy_sharp_latents, context, time_embedding)\n",
    "        \n",
    "        # 손실 계산 (예측된 노이즈와 실제 노이즈 간의 차이)\n",
    "        mse_loss  = torch.nn.functional.mse_loss(predicted_noise, noise)\n",
    "        \n",
    "        # Perceptual Loss 계산\n",
    "        # Decoder를 통해 latent를 이미지로 복원\n",
    "        predicted_sharp_latents = noisy_sharp_latents - predicted_noise\n",
    "        predicted_images = self.decoder(predicted_sharp_latents).clamp(0, 1)\n",
    "\n",
    "        # target_images = self.decoder(noisy_sharp_latents).clamp(0, 1) # sharp image만을 사용하기 위해 주석처리 및 아래로 수정\n",
    "        target_images = self.decoder(sharp_latents).clamp(0, 1)\n",
    "\n",
    "        perceptual_loss = self.perceptual_loss_fn(predicted_images, target_images)\n",
    "\n",
    "        # 손실 결합\n",
    "        lambda_p = 0.1  # Perceptual loss에 대한 가중치\n",
    "        combined_loss = mse_loss + lambda_p * perceptual_loss\n",
    "\n",
    "        return combined_loss\n",
    "\n",
    "    \n",
    "    def save_model(self, epoch):\n",
    "        # 모델과 옵티마이저 상태를 지정된 경로에 저장\n",
    "        model_path = os.path.join(self.save_path, f\"model_epoch_{epoch}.pth\")\n",
    "\n",
    "      \n",
    "        optimizer_path = os.path.join(self.save_path, f\"optimizer_epoch_{epoch}.pth\")\n",
    "        \n",
    "        torch.save(self.diffusion.state_dict(), model_path)\n",
    "        torch.save(self.optimizer.state_dict(), optimizer_path)\n",
    "        \n",
    "        print(f\"Model and optimizer saved at epoch {epoch} in {self.save_path}\")\n",
    "\n",
    "\n",
    "    def train(self, dataloader, num_epochs):\n",
    "        for epoch in range(num_epochs):\n",
    "            total_loss = 0\n",
    "            for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "                blur_images, sharp_images = batch\n",
    "                blur_latents, sharp_latents, context = self.prepare_batch(batch)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                loss = self.train_step(blur_latents, sharp_latents, context)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                                \n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            avg_loss = total_loss / len(dataloader)\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "            self.writer.add_scalar(\"Loss/Train\", avg_loss, epoch)\n",
    "\n",
    "            # 모델과 옵티마이저 저장\n",
    "            self.save_model(epoch + 1)\n",
    "\n",
    "            # 샘플 블러 이미지를 선택하여 디블러링 성능을 시각화\n",
    "            # sample_blur_image = blur_images[0]  # 첫 번째 이미지 선택\n",
    "            # self.log_generated_images(epoch + 1, sample_blur_image)\n",
    "\n",
    "    from PIL import Image\n",
    "\n",
    "    def log_generated_images(self, epoch, blur_image, output_dir=\"./generated_outputs\"):\n",
    "        \"\"\"\n",
    "        디블러링 과정을 수행하고 TensorBoard에 기록 및 결과 저장\n",
    "        \"\"\"\n",
    "        output_image = pipeline.generate(\n",
    "            prompt=\"Deblur image\",\n",
    "            uncond_prompt=\"\",\n",
    "            input_image=blur_image,\n",
    "            strength=0.5,\n",
    "            do_cfg=True,\n",
    "            cfg_scale=8,\n",
    "            sampler_name=\"ddpm\",\n",
    "            n_inference_steps=50,\n",
    "            seed=42,\n",
    "            models=self.models,\n",
    "            device=self.device,\n",
    "            idle_device=\"cpu\",\n",
    "            tokenizer=self.tokenizer,\n",
    "        )\n",
    "        # Convert output to PIL image\n",
    "        deblurred_image = Image.fromarray(output_image)\n",
    "\n",
    "        # Resize blur image to match deblurred image size\n",
    "        blur_image_resized = blur_image.resize(deblurred_image.size)\n",
    "\n",
    "        # Save the blurred and deblurred images\n",
    "        blur_image_resized.save(os.path.join(output_dir, f\"blurred_epoch_{epoch}.png\"))\n",
    "        deblurred_image.save(os.path.join(output_dir, f\"deblurred_epoch_{epoch}.png\"))\n",
    "\n",
    "        # Log images to TensorBoard\n",
    "        blur_tensor = TF.to_tensor(blur_image_resized).unsqueeze(0)  # (1, C, H, W)\n",
    "        deblurred_tensor = TF.to_tensor(deblurred_image).unsqueeze(0)  # (1, C, H, W)\n",
    "\n",
    "        self.writer.add_image(f\"Blurred_Image/Epoch_{epoch}\", blur_tensor, epoch)\n",
    "        self.writer.add_image(f\"Deblurred_Image/Epoch_{epoch}\", deblurred_tensor, epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fall/anaconda3/envs/ldms_311/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/fall/anaconda3/envs/ldms_311/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Epoch 1/100: 100%|██████████| 2103/2103 [55:23<00:00,  1.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Average Loss: 2.2096\n",
      "Model and optimizer saved at epoch 1 in ./checkpoints\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:   2%|▏         | 50/2103 [01:18<53:59,  1.58s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 학습\u001b[39;00m\n\u001b[1;32m      4\u001b[0m fine_tuner \u001b[38;5;241m=\u001b[39m LDMFineTuner(models, tokenizer, device)\n\u001b[0;32m----> 5\u001b[0m fine_tuner\u001b[38;5;241m.\u001b[39mtrain(train_loader, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 121\u001b[0m, in \u001b[0;36mLDMFineTuner.train\u001b[0;34m(self, dataloader, num_epochs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    120\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_step(blur_latents, sharp_latents, context)\n\u001b[0;32m--> 121\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    124\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/envs/ldms_311/lib/python3.11/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    582\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    583\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/ldms_311/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[1;32m    348\u001b[0m     tensors,\n\u001b[1;32m    349\u001b[0m     grad_tensors_,\n\u001b[1;32m    350\u001b[0m     retain_graph,\n\u001b[1;32m    351\u001b[0m     create_graph,\n\u001b[1;32m    352\u001b[0m     inputs,\n\u001b[1;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    355\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/ldms_311/lib/python3.11/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%env PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n",
    "\n",
    "# 학습\n",
    "fine_tuner = LDMFineTuner(models, tokenizer, device)\n",
    "fine_tuner.train(train_loader, num_epochs=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ldms_311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
